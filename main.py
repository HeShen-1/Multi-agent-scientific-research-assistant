"""
å¤šæ™ºèƒ½ä½“ç§‘ç ”åŠ©æ‰‹ä¸»ç¨‹åº
Multi-Agent Research Assistant Main Program

åŸºäºCrewAIçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œè‡ªåŠ¨åŒ–ç”Ÿæˆç»“æ„åŒ–ç ”ç©¶æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    python main.py

ä½œè€…ï¼šAI Assistant
ç‰ˆæœ¬ï¼š1.0
"""

import os
import sys
import logging
import asyncio
from datetime import datetime
from dotenv import load_dotenv
from typing import List, Dict, Any
import concurrent.futures

# å¯¼å…¥ç½‘ç»œå·¥å…·
from utils.network_utils import (
    retry_with_backoff, 
    async_retry_with_backoff,
    check_api_connectivity,
    get_optimal_timeout,
    api_rate_limiter
)

# å¯¼å…¥CrewAIæ ¸å¿ƒç»„ä»¶
from crewai import Crew, Process

# é…ç½®DeepSeek API
def setup_deepseek_api():
    """é…ç½®DeepSeek APIè®¾ç½®"""
    api_base = os.getenv('OPENAI_API_BASE')
    if api_base:
        # ç¡®ä¿APIåŸºç¡€URLæ ¼å¼æ­£ç¡®
        if not api_base.endswith('/v1') and not api_base.endswith('/'):
            api_base = api_base.rstrip('/') + '/v1'
        os.environ['OPENAI_API_BASE'] = api_base
        logger.info(f"ä½¿ç”¨DeepSeek API: {api_base}")
    else:
        # é»˜è®¤ä½¿ç”¨DeepSeek API
        os.environ['OPENAI_API_BASE'] = "https://api.deepseek.com/v1"
        logger.info("ä½¿ç”¨é»˜è®¤DeepSeek APIç«¯ç‚¹")
    
    # è®¾ç½®é»˜è®¤æ¨¡å‹
    model_name = os.getenv('OPENAI_MODEL_NAME', 'deepseek-chat')
    os.environ['OPENAI_MODEL_NAME'] = model_name
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
    
    # éªŒè¯APIå¯†é’¥
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key or api_key == 'sk-your-deepseek-api-key-here':
        logger.error("DeepSeek APIå¯†é’¥æœªæ­£ç¡®è®¾ç½®ï¼")
        logger.error("è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ­£ç¡®çš„OPENAI_API_KEY")
        return False
    
    # æ£€æŸ¥APIè¿é€šæ€§ï¼ˆå¯é€‰ï¼Œé¿å…å¯åŠ¨æ—¶é˜»å¡ï¼‰
    try:
        api_base = os.getenv('OPENAI_API_BASE', 'https://api.deepseek.com/v1')
        if check_api_connectivity(api_base, api_key):
            logger.info("âœ… APIè¿é€šæ€§éªŒè¯æˆåŠŸ")
        else:
            logger.warning("âš ï¸ APIè¿é€šæ€§éªŒè¯å¤±è´¥ï¼Œä½†å°†ç»§ç»­è¿è¡Œ")
            logger.warning("å¦‚æœé‡åˆ°è¿æ¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œå’ŒAPIå¯†é’¥")
    except Exception as e:
        logger.warning(f"âš ï¸ APIè¿é€šæ€§æ£€æŸ¥è·³è¿‡: {e}")
    
    return True

# å¯¼å…¥è‡ªå®šä¹‰æ™ºèƒ½ä½“å’Œä»»åŠ¡
from agents.research_agents import (
    create_research_manager,
    create_senior_researcher,
    create_research_analyst,
    create_validator_agent
)
from tasks.research_tasks import (
    create_planning_task,
    create_research_execution_task,
    create_analysis_task,
    create_validation_task
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('research_assistant.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


def load_environment():
    """
    åŠ è½½ç¯å¢ƒå˜é‡å¹¶éªŒè¯å¿…è¦çš„APIå¯†é’¥
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸåŠ è½½æ‰€æœ‰å¿…è¦çš„ç¯å¢ƒå˜é‡
    """
    # åŠ è½½.envæ–‡ä»¶
    load_dotenv()
    
    # æ£€æŸ¥.envæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists('.env'):
        logger.error("âŒ æœªæ‰¾åˆ°.envé…ç½®æ–‡ä»¶ï¼")
        logger.error("è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åˆ›å»ºé…ç½®æ–‡ä»¶ï¼š")
        logger.error("1. å¤åˆ¶ env_example.txt ä¸º .env")
        logger.error("2. åœ¨ .env ä¸­è®¾ç½®æ‚¨çš„ DeepSeek API å¯†é’¥")
        logger.error("3. å°† OPENAI_API_KEY çš„å€¼æ›¿æ¢ä¸ºæ‚¨çš„çœŸå®APIå¯†é’¥")
        return False
    
    # é…ç½®DeepSeek API
    if not setup_deepseek_api():
        return False
    
    # æ£€æŸ¥å¯é€‰çš„Tavily APIå¯†é’¥
    tavily_key = os.getenv('TAVILY_API_KEY')
    if not tavily_key or tavily_key == 'tvly-your-tavily-api-key-here':
        logger.warning("æœªé…ç½®Tavily APIå¯†é’¥ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æœç´¢åŠŸèƒ½")
    
    logger.info("âœ… ç¯å¢ƒå˜é‡åŠ è½½æˆåŠŸ")
    return True


def validate_topic(topic: str) -> bool:
    """
    éªŒè¯ç ”ç©¶ä¸»é¢˜çš„æœ‰æ•ˆæ€§
    
    Args:
        topic (str): ç ”ç©¶ä¸»é¢˜
        
    Returns:
        bool: ä¸»é¢˜æ˜¯å¦æœ‰æ•ˆ
    """
    if not topic or len(topic.strip()) < 5:
        logger.error("ç ”ç©¶ä¸»é¢˜è¿‡çŸ­ï¼Œè¯·æä¾›æ›´è¯¦ç»†çš„ä¸»é¢˜æè¿°")
        return False
    
    if len(topic) > 500:
        logger.error("ç ”ç©¶ä¸»é¢˜è¿‡é•¿ï¼Œè¯·ç®€åŒ–ä¸»é¢˜æè¿°")
        return False
    
    return True


def save_report(content: str, topic: str) -> str:
    """
    ä¿å­˜ç ”ç©¶æŠ¥å‘Šåˆ°æ–‡ä»¶
    
    Args:
        content (str): æŠ¥å‘Šå†…å®¹
        topic (str): ç ”ç©¶ä¸»é¢˜
        
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    # åˆ›å»ºæŠ¥å‘Šç›®å½•
    reports_dir = "reports"
    if not os.path.exists(reports_dir):
        os.makedirs(reports_dir)
    
    # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨æ—¶é—´æˆ³é¿å…é‡å¤ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    topic_safe = "".join(c for c in topic[:50] if c.isalnum() or c in (' ', '-', '_')).strip()
    filename = f"research_report_{topic_safe}_{timestamp}.md"
    filepath = os.path.join(reports_dir, filename)
    
    # ä¿å­˜æ–‡ä»¶
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        logger.info(f"ç ”ç©¶æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    except Exception as e:
        logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}")
        return ""


def create_research_crew() -> Crew:
    """
    åˆ›å»ºå¤šæ™ºèƒ½ä½“ç ”ç©¶å›¢é˜Ÿ
    
    Returns:
        Crew: é…ç½®å¥½çš„CrewAIå›¢é˜Ÿå®ä¾‹
    """
    logger.info("æ­£åœ¨åˆ›å»ºæ™ºèƒ½ä½“å›¢é˜Ÿ...")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    research_manager = create_research_manager()
    senior_researcher = create_senior_researcher()
    research_analyst = create_research_analyst()
    
    logger.info("æ™ºèƒ½ä½“åˆ›å»ºå®Œæˆ")
    
    # åˆ›å»ºCrewï¼ˆæš‚æ—¶ä¸åŒ…å«ä»»åŠ¡ï¼Œä»»åŠ¡å°†åœ¨è¿è¡Œæ—¶åŠ¨æ€åˆ›å»ºï¼‰
    # æ™ºèƒ½ä½“å·²ç»åœ¨åˆ›å»ºæ—¶é…ç½®äº†LLMï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–é…ç½®
    
    # åˆ›å»ºCrewé…ç½®
    crew_config = {
        "agents": [research_manager, senior_researcher, research_analyst],
        "tasks": [],  # ä»»åŠ¡å°†åœ¨run_researchä¸­åŠ¨æ€æ·»åŠ 
        "process": Process.sequential,  # æš‚æ—¶ä½¿ç”¨é¡ºåºæµç¨‹é¿å…hierarchicalçš„å¤æ‚æ€§
        "verbose": True,  # è¯¦ç»†è¾“å‡ºæ¨¡å¼
        "memory": False,  # ç¦ç”¨å†…å­˜ä»¥é¿å…embedding APIè°ƒç”¨
        "max_iter": 3,  # æœ€å¤§è¿­ä»£æ¬¡æ•°
    }
    
    crew = Crew(**crew_config)
    
    logger.info("æ™ºèƒ½ä½“å›¢é˜Ÿåˆ›å»ºå®Œæˆ")
    return crew


def run_research(topic: str) -> str:
    """
    æ‰§è¡Œå®Œæ•´çš„ç ”ç©¶æµç¨‹ï¼ˆé»˜è®¤ä½¿ç”¨å¹¶å‘æ¨¡å¼ï¼‰
    
    Args:
        topic (str): ç ”ç©¶ä¸»é¢˜
        
    Returns:
        str: ç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šå†…å®¹
    """
    # é»˜è®¤ä½¿ç”¨é«˜æ€§èƒ½å¹¶å‘æ¨¡å¼
    return asyncio.run(run_parallel_research(topic))


def run_research_with_validation(topic: str) -> str:
    """
    æ‰§è¡Œå¸¦éªŒè¯åŠŸèƒ½çš„å®Œæ•´ç ”ç©¶æµç¨‹ï¼ˆæ¨èç”¨äºé«˜è´¨é‡æŠ¥å‘Šï¼‰
    
    Args:
        topic (str): ç ”ç©¶ä¸»é¢˜
        
    Returns:
        str: ç»è¿‡éªŒè¯çš„é«˜è´¨é‡ç ”ç©¶æŠ¥å‘Šå†…å®¹
    """
    return asyncio.run(run_parallel_research_with_validation(topic))


def run_research_sync(topic: str) -> str:
    """
    æ‰§è¡Œå®Œæ•´çš„ç ”ç©¶æµç¨‹ï¼ˆä¼ ç»ŸåŒæ­¥ç‰ˆæœ¬ï¼‰
    
    Args:
        topic (str): ç ”ç©¶ä¸»é¢˜
        
    Returns:
        str: ç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šå†…å®¹
    """
    logger.info(f"å¼€å§‹åŒæ­¥ç ”ç©¶ä¸»é¢˜: {topic}")
    start_time = datetime.now()
    
    try:
        # åˆ›å»ºæ™ºèƒ½ä½“å›¢é˜Ÿ
        crew = create_research_crew()
        
        # åŠ¨æ€åˆ›å»ºä»»åŠ¡
        logger.info("åˆ›å»ºç ”ç©¶ä»»åŠ¡...")
        
        # åˆ›å»ºä»»åŠ¡å¹¶æ·»åŠ åˆ°crewä¸­
        planning_task = create_planning_task(crew.agents[0], topic)  # Research Manager
        execution_task = create_research_execution_task(crew.agents[1], topic)  # Senior Researcher
        analysis_task = create_analysis_task(crew.agents[2], topic)  # Research Analyst
        
        # è®¾ç½®ä»»åŠ¡ä¾èµ–å…³ç³»
        execution_task.context = [planning_task]
        analysis_task.context = [execution_task]
        
        # å°†ä»»åŠ¡æ·»åŠ åˆ°crew
        crew.tasks = [planning_task, execution_task, analysis_task]
        
        logger.info("å¼€å§‹æ‰§è¡ŒåŒæ­¥ç ”ç©¶æµç¨‹...")
        
        # æ‰§è¡Œç ”ç©¶
        result = crew.kickoff(inputs={"topic": topic})
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        logger.info(f"åŒæ­¥ç ”ç©¶å®Œæˆï¼Œæ€»è€—æ—¶: {execution_time:.2f}ç§’")
        
        # æ·»åŠ å…ƒæ•°æ®åˆ°æŠ¥å‘Š
        metadata = f"""
---
ç ”ç©¶ä¸»é¢˜: {topic}
ç”Ÿæˆæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}
æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’
æ‰§è¡Œæ¨¡å¼: ä¼ ç»ŸåŒæ­¥æ¨¡å¼
ç³»ç»Ÿç‰ˆæœ¬: Multi-Agent Research Assistant v1.1 (Sync)
---

"""
        
        final_report = metadata + str(result)
        
        # è‡ªåŠ¨ä¿å­˜åˆ°è®°å¿†åº“
        try:
            from tools.memory_tool import get_memory_manager
            memory_manager = get_memory_manager()
            memory_id = memory_manager.store_research(
                topic=topic,
                content=final_report,
                metadata={
                    "execution_mode": "åŒæ­¥æ¨¡å¼",
                    "execution_time": f"{execution_time:.2f}ç§’",
                    "version": "v1.1"
                }
            )
            if memory_id:
                logger.info(f"ç ”ç©¶å·²è‡ªåŠ¨ä¿å­˜åˆ°è®°å¿†åº“: {memory_id}")
        except Exception as e:
            logger.warning(f"è‡ªåŠ¨ä¿å­˜è®°å¿†å¤±è´¥: {str(e)}")
        
        return final_report
        
    except Exception as e:
        logger.error(f"åŒæ­¥ç ”ç©¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        return f"# ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥\n\né”™è¯¯ä¿¡æ¯: {str(e)}\n\nè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥åé‡è¯•ã€‚"


async def run_research_async(topic: str) -> str:
    """
    æ‰§è¡Œå®Œæ•´çš„ç ”ç©¶æµç¨‹ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
    
    Args:
        topic (str): ç ”ç©¶ä¸»é¢˜
        
    Returns:
        str: ç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šå†…å®¹
    """
    logger.info(f"å¼€å§‹å¼‚æ­¥ç ”ç©¶ä¸»é¢˜: {topic}")
    start_time = datetime.now()
    
    try:
        # åˆ›å»ºæ™ºèƒ½ä½“å›¢é˜Ÿ
        crew = create_research_crew()
        
        # åŠ¨æ€åˆ›å»ºä»»åŠ¡
        logger.info("åˆ›å»ºç ”ç©¶ä»»åŠ¡...")
        
        # åˆ›å»ºä»»åŠ¡å¹¶æ·»åŠ åˆ°crewä¸­
        planning_task = create_planning_task(crew.agents[0], topic)  # Research Manager
        execution_task = create_research_execution_task(crew.agents[1], topic)  # Senior Researcher
        analysis_task = create_analysis_task(crew.agents[2], topic)  # Research Analyst
        
        # è®¾ç½®ä»»åŠ¡ä¾èµ–å…³ç³»
        execution_task.context = [planning_task]
        analysis_task.context = [execution_task]
        
        # å°†ä»»åŠ¡æ·»åŠ åˆ°crew
        crew.tasks = [planning_task, execution_task, analysis_task]
        
        logger.info("å¼€å§‹æ‰§è¡Œå¼‚æ­¥ç ”ç©¶æµç¨‹...")
        
        # ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œç ”ç©¶
        result = await run_crew_async(crew, {"topic": topic})
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        logger.info(f"å¼‚æ­¥ç ”ç©¶å®Œæˆï¼Œæ€»è€—æ—¶: {execution_time:.2f}ç§’")
        
        # æ·»åŠ å…ƒæ•°æ®åˆ°æŠ¥å‘Š
        metadata = f"""
---
ç ”ç©¶ä¸»é¢˜: {topic}
ç”Ÿæˆæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}
æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’
æ‰§è¡Œæ¨¡å¼: å¼‚æ­¥å¹¶å‘æ¨¡å¼
ç³»ç»Ÿç‰ˆæœ¬: Multi-Agent Research Assistant v1.1 (Async)
---

"""
        
        final_report = metadata + str(result)
        
        # è‡ªåŠ¨ä¿å­˜åˆ°è®°å¿†åº“
        try:
            from tools.memory_tool import get_memory_manager
            memory_manager = get_memory_manager()
            memory_id = memory_manager.store_research(
                topic=topic,
                content=final_report,
                metadata={
                    "execution_mode": "å¼‚æ­¥æ¨¡å¼",
                    "execution_time": f"{execution_time:.2f}ç§’",
                    "version": "v1.1"
                }
            )
            if memory_id:
                logger.info(f"ç ”ç©¶å·²è‡ªåŠ¨ä¿å­˜åˆ°è®°å¿†åº“: {memory_id}")
        except Exception as e:
            logger.warning(f"è‡ªåŠ¨ä¿å­˜è®°å¿†å¤±è´¥: {str(e)}")
        
        return final_report
        
    except Exception as e:
        logger.error(f"å¼‚æ­¥ç ”ç©¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        return f"# ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥\n\né”™è¯¯ä¿¡æ¯: {str(e)}\n\nè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥åé‡è¯•ã€‚"


# ç§»é™¤è£…é¥°å™¨ï¼Œæ”¹ä¸ºå†…éƒ¨å¤„ç†é‡è¯•
async def run_crew_async(crew: Crew, inputs: Dict[str, Any]) -> str:
    """
    å¼‚æ­¥è¿è¡ŒCrewAIå›¢é˜Ÿï¼ˆå¸¦å¼ºåŒ–é‡è¯•æœºåˆ¶ï¼‰
    
    Args:
        crew: CrewAIå›¢é˜Ÿå®ä¾‹
        inputs: è¾“å…¥å‚æ•°
        
    Returns:
        str: æ‰§è¡Œç»“æœ
    """
    max_retries = 5
    base_delay = 2.0
    max_delay = 60.0
    
    for attempt in range(max_retries + 1):
        try:
            # åº”ç”¨é€Ÿç‡é™åˆ¶
            api_rate_limiter.wait_if_needed()
            
            logger.info("å¼€å§‹æ‰§è¡ŒCrewä»»åŠ¡...")
            
            # CrewAI 0.28.8+ æ”¯æŒå¼‚æ­¥æ‰§è¡Œ
            if hasattr(crew, 'kickoff_async'):
                logger.info("ä½¿ç”¨CrewAIåŸç”Ÿå¼‚æ­¥æ‰§è¡Œ")
                result = await crew.kickoff_async(inputs=inputs)
            else:
                # å¦‚æœä¸æ”¯æŒåŸç”Ÿå¼‚æ­¥ï¼Œä½¿ç”¨çº¿ç¨‹æ± åŒ…è£…
                logger.info("ä½¿ç”¨çº¿ç¨‹æ± åŒ…è£…åŒæ­¥æ‰§è¡Œ")
                with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(
                        executor, 
                        lambda: crew.kickoff(inputs=inputs)
                    )
            
            logger.info("âœ… Crewæ‰§è¡ŒæˆåŠŸ")
            return result
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ Crewæ‰§è¡Œå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}): {error_msg}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œç›¸å…³é”™è¯¯
            network_errors = [
                'connection error', 'timeout', 'cloudflare', 
                'just a moment', 'rate limit', 'too many requests',
                'internal server error', 'bad gateway', 'service unavailable'
            ]
            
            is_network_error = any(error in error_msg.lower() for error in network_errors)
            
            if attempt < max_retries and is_network_error:
                delay = min(base_delay * (2 ** attempt), max_delay)
                logger.info(f"ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                await asyncio.sleep(delay)
                continue
            else:
                # éç½‘ç»œé”™è¯¯æˆ–å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                logger.error(f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæœ€åçš„é”™è¯¯: {error_msg}")
                raise e


async def run_parallel_research(topic: str) -> str:
    """
    æ‰§è¡Œå¹¶å‘ä¼˜åŒ–çš„ç ”ç©¶æµç¨‹
    
    è¿™ä¸ªç‰ˆæœ¬å®ç°äº†çœŸæ­£çš„å¹¶è¡Œæœç´¢ï¼š
    - Webæœç´¢å’ŒarXivæœç´¢åŒæ—¶è¿›è¡Œ
    - å¤šä¸ªæŸ¥è¯¢å¹¶å‘æ‰§è¡Œ
    - æœ€å¤§åŒ–åˆ©ç”¨I/Oç­‰å¾…æ—¶é—´
    
    Args:
        topic (str): ç ”ç©¶ä¸»é¢˜
        
    Returns:
        str: ç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šå†…å®¹
    """
    logger.info(f"å¼€å§‹å¹¶å‘ç ”ç©¶ä¸»é¢˜: {topic}")
    start_time = datetime.now()
    
    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šè§„åˆ’é˜¶æ®µï¼ˆå¿…é¡»ä¸²è¡Œï¼‰
        planning_crew = create_planning_crew(topic)
        planning_result = await run_crew_async(planning_crew, {"topic": topic})
        
        logger.info("è§„åˆ’é˜¶æ®µå®Œæˆï¼Œå¼€å§‹å¹¶å‘æœç´¢...")
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¹¶å‘æœç´¢é˜¶æ®µ
        web_search_crew = create_web_search_crew(topic)
        arxiv_search_crew = create_arxiv_search_crew(topic)
        
        # å°† CrewOutput è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        planning_context_str = str(planning_result)
        
        # å¹¶å‘æ‰§è¡ŒWebæœç´¢å’ŒarXivæœç´¢
        web_task = run_crew_async(web_search_crew, {
            "topic": topic,
            "planning_context": planning_context_str
        })
        arxiv_task = run_crew_async(arxiv_search_crew, {
            "topic": topic, 
            "planning_context": planning_context_str
        })
        
        # ç­‰å¾…æ‰€æœ‰æœç´¢ä»»åŠ¡å®Œæˆ
        web_results, arxiv_results = await asyncio.gather(web_task, arxiv_task)
        
        logger.info("å¹¶å‘æœç´¢é˜¶æ®µå®Œæˆï¼Œå¼€å§‹åˆ†æ...")
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šåˆ†æé˜¶æ®µï¼ˆä¸²è¡Œï¼‰
        analysis_crew = create_analysis_crew(topic)
        final_result = await run_crew_async(analysis_crew, {
            "topic": topic,
            "web_results": str(web_results),
            "arxiv_results": str(arxiv_results)
        })
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        logger.info(f"å¹¶å‘ç ”ç©¶å®Œæˆï¼Œæ€»è€—æ—¶: {execution_time:.2f}ç§’")
        
        # æ·»åŠ å…ƒæ•°æ®åˆ°æŠ¥å‘Š
        metadata = f"""
---
ç ”ç©¶ä¸»é¢˜: {topic}
ç”Ÿæˆæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}
æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’
æ‰§è¡Œæ¨¡å¼: é«˜çº§å¹¶å‘æ¨¡å¼ (Web + arXiv å¹¶è¡Œæœç´¢)
ç³»ç»Ÿç‰ˆæœ¬: Multi-Agent Research Assistant v1.1 (Advanced Parallel)
---

"""
        
        final_report = metadata + str(final_result)
        
        # è‡ªåŠ¨ä¿å­˜åˆ°è®°å¿†åº“
        try:
            from tools.memory_tool import get_memory_manager
            memory_manager = get_memory_manager()
            memory_id = memory_manager.store_research(
                topic=topic,
                content=final_report,
                metadata={
                    "execution_mode": "é«˜çº§å¹¶å‘æ¨¡å¼",
                    "execution_time": f"{execution_time:.2f}ç§’",
                    "version": "v1.1",
                    "parallel_search": True
                }
            )
            if memory_id:
                logger.info(f"ç ”ç©¶å·²è‡ªåŠ¨ä¿å­˜åˆ°è®°å¿†åº“: {memory_id}")
        except Exception as e:
            logger.warning(f"è‡ªåŠ¨ä¿å­˜è®°å¿†å¤±è´¥: {str(e)}")
        
        return final_report
        
    except Exception as e:
        logger.error(f"å¹¶å‘ç ”ç©¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        return f"# ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥\n\né”™è¯¯ä¿¡æ¯: {str(e)}\n\nè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥åé‡è¯•ã€‚"


def create_planning_crew(topic: str) -> Crew:
    """åˆ›å»ºä¸“é—¨ç”¨äºè§„åˆ’çš„å›¢é˜Ÿ"""
    research_manager = create_research_manager()
    planning_task = create_planning_task(research_manager, topic)
    
    crew = Crew(
        agents=[research_manager],
        tasks=[planning_task],
        process=Process.sequential,
        verbose=True,
        memory=False
    )
    return crew


def create_web_search_crew(topic: str) -> Crew:
    """åˆ›å»ºä¸“é—¨ç”¨äºWebæœç´¢çš„å›¢é˜Ÿ"""
    from tasks.research_tasks import create_web_search_task
    
    web_researcher = create_senior_researcher()
    web_search_task = create_web_search_task(web_researcher, topic)
    
    crew = Crew(
        agents=[web_researcher],
        tasks=[web_search_task],
        process=Process.sequential,
        verbose=True,
        memory=False
    )
    return crew


def create_arxiv_search_crew(topic: str) -> Crew:
    """åˆ›å»ºä¸“é—¨ç”¨äºarXivæœç´¢çš„å›¢é˜Ÿ"""
    from tasks.research_tasks import create_arxiv_search_task
    
    arxiv_researcher = create_senior_researcher()
    arxiv_search_task = create_arxiv_search_task(arxiv_researcher, topic)
    
    crew = Crew(
        agents=[arxiv_researcher],
        tasks=[arxiv_search_task],
        process=Process.sequential,
        verbose=True,
        memory=False
    )
    return crew


def create_analysis_crew(topic: str) -> Crew:
    """åˆ›å»ºä¸“é—¨ç”¨äºåˆ†æçš„å›¢é˜Ÿ"""
    from tasks.research_tasks_fixed import create_integrated_analysis_task_fixed
    
    research_analyst = create_research_analyst()
    analysis_task = create_integrated_analysis_task_fixed(research_analyst, topic)
    
    crew = Crew(
        agents=[research_analyst],
        tasks=[analysis_task],
        process=Process.sequential,
        verbose=True,
        memory=False
    )
    return crew


def create_validation_crew(topic: str, research_report: str = "") -> Crew:
    """åˆ›å»ºä¸“é—¨ç”¨äºéªŒè¯çš„å›¢é˜Ÿ"""
    validator = create_validator_agent()
    validation_task = create_validation_task(validator, topic, research_report)
    
    crew = Crew(
        agents=[validator],
        tasks=[validation_task],
        process=Process.sequential,
        verbose=True,
        memory=False
    )
    return crew


async def run_parallel_research_with_validation(topic: str) -> str:
    """
    æ‰§è¡Œå¸¦éªŒè¯åŠŸèƒ½çš„å¹¶å‘ç ”ç©¶æµç¨‹
    
    å®ç°å®¡æŸ¥å¾ªç¯æœºåˆ¶ï¼š
    1. è§„åˆ’é˜¶æ®µï¼ˆä¸²è¡Œï¼‰
    2. å¹¶å‘æœç´¢é˜¶æ®µï¼ˆWeb + arXiv å¹¶è¡Œï¼‰
    3. åˆ†æé˜¶æ®µï¼ˆä¸²è¡Œï¼‰
    4. éªŒè¯é˜¶æ®µï¼ˆä¸²è¡Œï¼‰
    5. å¦‚æœéªŒè¯ä¸é€šè¿‡ï¼Œè¿”å›åˆ†æé˜¶æ®µé‡æ–°ç”ŸæˆæŠ¥å‘Šï¼ˆæœ€å¤š2æ¬¡é‡è¯•ï¼‰
    
    Args:
        topic (str): ç ”ç©¶ä¸»é¢˜
        
    Returns:
        str: ç»è¿‡éªŒè¯çš„é«˜è´¨é‡ç ”ç©¶æŠ¥å‘Šå†…å®¹
    """
    logger.info(f"å¼€å§‹å¸¦éªŒè¯çš„å¹¶å‘ç ”ç©¶ä¸»é¢˜: {topic}")
    start_time = datetime.now()
    
    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šè§„åˆ’é˜¶æ®µï¼ˆå¿…é¡»ä¸²è¡Œï¼‰
        logger.info("é˜¶æ®µ1: ç ”ç©¶è§„åˆ’...")
        planning_crew = create_planning_crew(topic)
        planning_result = await run_crew_async(planning_crew, {"topic": topic})
        
        logger.info("è§„åˆ’é˜¶æ®µå®Œæˆï¼Œå¼€å§‹å¹¶å‘æœç´¢...")
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¹¶å‘æœç´¢é˜¶æ®µ
        logger.info("é˜¶æ®µ2: å¹¶å‘ä¿¡æ¯æœé›†...")
        web_search_crew = create_web_search_crew(topic)
        arxiv_search_crew = create_arxiv_search_crew(topic)
        
        # å°† CrewOutput è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        planning_context_str = str(planning_result)
        
        # å¹¶å‘æ‰§è¡ŒWebæœç´¢å’ŒarXivæœç´¢
        web_task = run_crew_async(web_search_crew, {
            "topic": topic,
            "planning_context": planning_context_str
        })
        arxiv_task = run_crew_async(arxiv_search_crew, {
            "topic": topic, 
            "planning_context": planning_context_str
        })
        
        # ç­‰å¾…æ‰€æœ‰æœç´¢ä»»åŠ¡å®Œæˆ
        web_results, arxiv_results = await asyncio.gather(web_task, arxiv_task)
        
        logger.info("å¹¶å‘æœç´¢é˜¶æ®µå®Œæˆï¼Œå¼€å§‹åˆ†æå’ŒéªŒè¯å¾ªç¯...")
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šåˆ†æå’ŒéªŒè¯å¾ªç¯ï¼ˆæœ€å¤š3æ¬¡å°è¯•ï¼‰
        max_validation_attempts = 3
        validation_passed = False
        final_report = None
        validation_feedback = ""
        
        for attempt in range(max_validation_attempts):
            logger.info(f"é˜¶æ®µ3.{attempt+1}: åˆ†æé˜¶æ®µï¼ˆå°è¯• {attempt+1}/{max_validation_attempts}ï¼‰...")
            
            # åˆ†æé˜¶æ®µ - ä½¿ç”¨ä¿®å¤ç‰ˆæœ¬çš„å‡½æ•°
            from tasks.research_tasks_fixed import create_integrated_analysis_task_fixed
            research_analyst = create_research_analyst()
            
            # æ ¹æ®æ˜¯å¦æœ‰éªŒè¯åé¦ˆåˆ›å»ºä¸åŒçš„ä»»åŠ¡
            if attempt > 0:
                analysis_task = create_integrated_analysis_task_fixed(research_analyst, topic, validation_feedback)
                logger.info(f"åŸºäºéªŒè¯åé¦ˆè¿›è¡ŒæŠ¥å‘Šæ”¹è¿›...")
            else:
                analysis_task = create_integrated_analysis_task_fixed(research_analyst, topic)
            
            analysis_crew = Crew(
                agents=[research_analyst],
                tasks=[analysis_task],
                process=Process.sequential,
                verbose=True,
                memory=False
            )
            
            analysis_inputs = {
                "topic": topic,
                "web_results": str(web_results),
                "arxiv_results": str(arxiv_results)
            }
            
            analysis_result = await run_crew_async(analysis_crew, analysis_inputs)
            
            logger.info(f"é˜¶æ®µ4.{attempt+1}: éªŒè¯é˜¶æ®µï¼ˆå°è¯• {attempt+1}/{max_validation_attempts}ï¼‰...")
            
            # éªŒè¯é˜¶æ®µ
            validation_crew = create_validation_crew(topic, str(analysis_result))
            validation_result = await run_crew_async(validation_crew, {
                "topic": topic,
                "research_report": str(analysis_result)
            })
            
            # è§£æéªŒè¯ç»“æœ
            validation_result_str = str(validation_result)
            
            # ç®€å•çš„éªŒè¯ç»“æœè§£æï¼ˆå®é™…é¡¹ç›®ä¸­å¯ä»¥æ›´å¤æ‚ï¼‰
            if "éªŒè¯é€šè¿‡" in validation_result_str or "è´¨é‡è¯„çº§ï¼šä¼˜ç§€" in validation_result_str or "è´¨é‡è¯„çº§ï¼šè‰¯å¥½" in validation_result_str:
                validation_passed = True
                final_report = str(analysis_result)
                logger.info(f"âœ… éªŒè¯é€šè¿‡ï¼ï¼ˆå°è¯• {attempt+1}/{max_validation_attempts}ï¼‰")
                break
            else:
                logger.info(f"âš ï¸ éªŒè¯æœªé€šè¿‡ï¼ˆå°è¯• {attempt+1}/{max_validation_attempts}ï¼‰ï¼Œå‡†å¤‡æ”¹è¿›...")
                validation_feedback = validation_result_str
                if attempt == max_validation_attempts - 1:
                    # æœ€åä¸€æ¬¡å°è¯•ï¼Œå³ä½¿éªŒè¯æœªé€šè¿‡ä¹Ÿä½¿ç”¨æœ€åçš„æŠ¥å‘Š
                    final_report = str(analysis_result)
                    logger.warning("å·²è¾¾åˆ°æœ€å¤§éªŒè¯å°è¯•æ¬¡æ•°ï¼Œä½¿ç”¨æœ€ç»ˆç‰ˆæœ¬æŠ¥å‘Š")
        
        if not final_report:
            raise Exception("æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        logger.info(f"å¸¦éªŒè¯çš„å¹¶å‘ç ”ç©¶å®Œæˆï¼Œæ€»è€—æ—¶: {execution_time:.2f}ç§’")
        
        # æ·»åŠ å…ƒæ•°æ®åˆ°æŠ¥å‘Š
        validation_status = "âœ… éªŒè¯é€šè¿‡" if validation_passed else "âš ï¸ éƒ¨åˆ†éªŒè¯"
        metadata = f"""
---
ç ”ç©¶ä¸»é¢˜: {topic}
ç”Ÿæˆæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}
æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’
æ‰§è¡Œæ¨¡å¼: é«˜çº§å¹¶å‘æ¨¡å¼ + è´¨é‡éªŒè¯å¾ªç¯
éªŒè¯çŠ¶æ€: {validation_status}
ç³»ç»Ÿç‰ˆæœ¬: Multi-Agent Research Assistant v1.2 (Advanced Parallel + Validation)
---

"""
        
        final_report_with_metadata = metadata + final_report
        
        # è‡ªåŠ¨ä¿å­˜åˆ°è®°å¿†åº“
        try:
            from tools.memory_tool import get_memory_manager
            memory_manager = get_memory_manager()
            memory_id = memory_manager.store_research(
                topic=topic,
                content=final_report_with_metadata,
                metadata={
                    "execution_mode": "é«˜çº§å¹¶å‘æ¨¡å¼+éªŒè¯",
                    "execution_time": f"{execution_time:.2f}ç§’",
                    "version": "v1.2",
                    "parallel_search": True,
                    "validation_enabled": True,
                    "validation_passed": validation_passed
                }
            )
            if memory_id:
                logger.info(f"ç ”ç©¶å·²è‡ªåŠ¨ä¿å­˜åˆ°è®°å¿†åº“: {memory_id}")
        except Exception as e:
            logger.warning(f"è‡ªåŠ¨ä¿å­˜è®°å¿†å¤±è´¥: {str(e)}")
        
        return final_report_with_metadata
        
    except Exception as e:
        logger.error(f"å¸¦éªŒè¯çš„å¹¶å‘ç ”ç©¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        return f"# ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥\n\né”™è¯¯ä¿¡æ¯: {str(e)}\n\nè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥åé‡è¯•ã€‚"


def interactive_mode():
    """
    äº¤äº’æ¨¡å¼ï¼Œå…è®¸ç”¨æˆ·è¾“å…¥ç ”ç©¶ä¸»é¢˜
    """
    print("\n" + "="*60)
    print("ğŸ¤– å¤šæ™ºèƒ½ä½“ç§‘ç ”åŠ©æ‰‹ (Multi-Agent Research Assistant)")
    print("="*60)
    print("è¿™æ˜¯ä¸€ä¸ªåŸºäºCrewAIçš„æ™ºèƒ½ç ”ç©¶ç³»ç»Ÿï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–ç ”ç©¶æŠ¥å‘Šã€‚")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç¨‹åºã€‚")
    print("="*60 + "\n")
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            topic = input("ğŸ“ è¯·è¾“å…¥æ‚¨æƒ³è¦ç ”ç©¶çš„ä¸»é¢˜: ").strip()
            
            # æ£€æŸ¥é€€å‡ºå‘½ä»¤
            if topic.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨å¤šæ™ºèƒ½ä½“ç§‘ç ”åŠ©æ‰‹ï¼")
                break
            
            # éªŒè¯ä¸»é¢˜
            if not validate_topic(topic):
                continue
            
            print(f"\nğŸš€ å¼€å§‹ç ”ç©¶ä¸»é¢˜: {topic}")
            
            # è¯¢é—®æ‰§è¡Œæ¨¡å¼
            mode_choice = input("ğŸ”§ é€‰æ‹©æ‰§è¡Œæ¨¡å¼ [1-æ ‡å‡†æ¨¡å¼, 2-å¼‚æ­¥æ¨¡å¼, 3-å¹¶å‘æ¨¡å¼, 4-éªŒè¯æ¨¡å¼] (é»˜è®¤:4): ").strip()
            if mode_choice == "1":
                print("â³ ä½¿ç”¨æ ‡å‡†åŒæ­¥æ¨¡å¼ï¼Œæ™ºèƒ½ä½“å›¢é˜Ÿæ­£åœ¨å·¥ä½œä¸­...")
                result = run_research_sync(topic)
            elif mode_choice == "2":
                print("âš¡ ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼Œæ™ºèƒ½ä½“å›¢é˜Ÿæ­£åœ¨é«˜æ•ˆå·¥ä½œä¸­...")
                result = asyncio.run(run_research_async(topic))
            elif mode_choice == "3":
                print("ğŸš€ ä½¿ç”¨é«˜çº§å¹¶å‘æ¨¡å¼ï¼Œæœ€å¤§åŒ–æ€§èƒ½...")
                result = asyncio.run(run_parallel_research(topic))
            else:
                print("ğŸ” ä½¿ç”¨éªŒè¯æ¨¡å¼ï¼Œç¡®ä¿æœ€é«˜è´¨é‡æŠ¥å‘Š...")
                result = asyncio.run(run_parallel_research_with_validation(topic))
            
            # ä¿å­˜æŠ¥å‘Š
            filepath = save_report(result, topic)
            
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            print("\n" + "="*60)
            print("âœ… ç ”ç©¶å®Œæˆï¼")
            if filepath:
                print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")
            print("="*60)
            
            # è¯¢é—®æ˜¯å¦æŸ¥çœ‹æŠ¥å‘Šæ‘˜è¦
            show_summary = input("\nğŸ“– æ˜¯å¦æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦ï¼Ÿ(y/n): ").strip().lower()
            if show_summary in ['y', 'yes', 'Y', 'æ˜¯']:
                # æ˜¾ç¤ºæŠ¥å‘Šçš„å‰500å­—ç¬¦
                print("\nğŸ“‹ æŠ¥å‘Šæ‘˜è¦:")
                print("-" * 40)
                print(result[:500] + "..." if len(result) > 500 else result)
                print("-" * 40)
            
            print("\n")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹åºå·²ä¸­æ–­ï¼Œæ„Ÿè°¢ä½¿ç”¨ï¼")
            break
        except Exception as e:
            logger.error(f"äº¤äº’æ¨¡å¼å‘ç”Ÿé”™è¯¯: {str(e)}")
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")


def main():
    """
    ä¸»å‡½æ•°
    """
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        if not load_environment():
            sys.exit(1)
        
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        if len(sys.argv) > 1:
            # å‘½ä»¤è¡Œæ¨¡å¼
            topic = " ".join(sys.argv[1:])
            if validate_topic(topic):
                result = run_research(topic)
                filepath = save_report(result, topic)
                print(f"ç ”ç©¶å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")
            else:
                print("æ— æ•ˆçš„ç ”ç©¶ä¸»é¢˜")
                sys.exit(1)
        else:
            # äº¤äº’æ¨¡å¼
            interactive_mode()
            
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œå‘ç”Ÿé”™è¯¯: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
